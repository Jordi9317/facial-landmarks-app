{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omu6HLrwGmqs"
      },
      "source": [
        "# Laboratorio 2: De Jupyter a Producción\n",
        "## Detección de Landmarks Faciales con Streamlit\n",
        "\n",
        "**Materia**: Procesamiento Digital de Imágenes e Introducción a Visión por Computadora  \n",
        "**Objetivo**: Convertir un notebook de análisis de landmarks faciales en una aplicación web interactiva desplegada en Streamlit Community Cloud\n",
        "\n",
        "---\n",
        "\n",
        "## Contexto Pedagógico\n",
        "\n",
        "En el **Laboratorio 1** trabajamos con:\n",
        "- Hugging Face Spaces\n",
        "- Modelos pre-entrenados\n",
        "- Entornos virtuales\n",
        "- Gradio para interfaces\n",
        "- Kilo como agente de desarrollo\n",
        "\n",
        "En este **Laboratorio 2** vamos a dar un paso más profesional:\n",
        "- Usar **VS Code** como IDE principal\n",
        "- Trabajar con **Kilo + Code Supernova 1M** como agente AI ([https://kilocode.ai/](https://kilocode.ai/))\n",
        "- Convertir notebooks en scripts Python modulares\n",
        "- Crear interfaces con **Streamlit** ([https://streamlit.io/](https://streamlit.io/))\n",
        "- Desplegar en **Streamlit Community Cloud**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSJysfXvGmqw"
      },
      "source": [
        "## Parte 1: Fundamentos Teóricos\n",
        "\n",
        "### ¿Qué son los Landmarks Faciales?\n",
        "\n",
        "Los **Landmarks Faciales** son puntos de referencia específicos que mapean las características clave de un rostro humano. Pensá en ellos como un \"esqueleto\" invisible que describe la geometría facial.\n",
        "\n",
        "![MediaPipe Face Mesh](https://ai.google.dev/static/mediapipe/images/solutions/face_landmarker_keypoints.png?hl=es-419)\n",
        "\n",
        "**Referencia oficial**: [MediaPipe Face Landmarker](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker)\n",
        "\n",
        "**Cantidad de puntos**: MediaPipe Face Mesh detecta **478 landmarks** que cubren:\n",
        "- Contorno facial (mandíbula, frente)\n",
        "- Ojos (iris, párpados, cejas)\n",
        "- Nariz (puente, fosas nasales)\n",
        "- Boca (labios, comisuras)\n",
        "\n",
        "### Aplicaciones en la Industria\n",
        "\n",
        "| Aplicación | Descripción | Ejemplo Real |\n",
        "|------------|-------------|-------------|\n",
        "| **Filtros AR** | Superponer objetos virtuales en tiempo real | Instagram, Snapchat, TikTok |\n",
        "| **Análisis de Expresiones** | Detectar emociones para UX testing | Microsoft Azure Emotion API |\n",
        "| **Animación Facial** | Captura de movimiento para CGI | Avatar, The Mandalorian |\n",
        "| **Autenticación** | Face ID, reconocimiento facial | Apple Face ID, bancos |\n",
        "| **Salud** | Detección de parálisis facial, análisis genético | Investigación médica |\n",
        "| **Gaming** | Control de personajes con expresiones | VR/AR gaming |\n",
        "\n",
        "### Stack Tecnológico\n",
        "\n",
        "```\n",
        "MediaPipe (Google) → Modelo pre-entrenado de detección\n",
        "    ↓\n",
        "OpenCV → Procesamiento de imágenes\n",
        "    ↓\n",
        "Streamlit → Interfaz web interactiva\n",
        "    ↓\n",
        "Streamlit Cloud → Hosting y deployment\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ZXAiyFGmqx"
      },
      "source": [
        "## Parte 2: Arquitectura del Proyecto\n",
        "\n",
        "### Estructura de Directorios (objetivo final)\n",
        "\n",
        "```\n",
        "facial-landmarks-app/\n",
        "│\n",
        "├── venv/                    # Entorno virtual (no subir a Git)\n",
        "│\n",
        "├── src/\n",
        "│   ├── detector.py          # Lógica de detección de landmarks\n",
        "│   ├── utils.py             # Funciones auxiliares (conversión de imágenes)\n",
        "│   └── config.py            # Configuración (parámetros del modelo)\n",
        "│\n",
        "├── app.py                   # Aplicación Streamlit principal\n",
        "│\n",
        "├── requirements.txt         # Dependencias del proyecto\n",
        "├── .gitignore              # Archivos a ignorar en Git\n",
        "└── README.md               # Documentación del proyecto\n",
        "```\n",
        "\n",
        "### Flujo de Trabajo\n",
        "\n",
        "El proceso de desarrollo sigue estas etapas:\n",
        "\n",
        "```\n",
        "1. Jupyter Notebook\n",
        "   ↓\n",
        "2. Análisis y Prototipado\n",
        "   ↓\n",
        "3. Extracción de Lógica\n",
        "   ↓\n",
        "4. Scripts Python Modulares\n",
        "   ↓\n",
        "5. Interfaz Streamlit\n",
        "   ↓\n",
        "6. Testing Local\n",
        "   ↓\n",
        "7. Deployment en Streamlit Cloud\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu--sEacGmqx"
      },
      "source": [
        "## Parte 3: Paso a Paso - Desarrollo del MVP\n",
        "\n",
        "### Fase 1: Configuración del Entorno de Desarrollo\n",
        "\n",
        "#### Paso 1.1: Instalación de VS Code y Extensiones\n",
        "\n",
        "**Descargar VS Code**: [https://code.visualstudio.com/](https://code.visualstudio.com/)\n",
        "\n",
        "**Extensiones necesarias**:\n",
        "```bash\n",
        "# Abrir VS Code y buscar en Extensions (Ctrl+Shift+X):\n",
        "1. Python (Microsoft)\n",
        "2. Jupyter (Microsoft)\n",
        "3. Pylance (Microsoft)\n",
        "4. GitLens (opcional pero recomendado)\n",
        "```\n",
        "\n",
        "#### Paso 1.2: Configuración de Kilo con Code Supernova\n",
        "\n",
        "**Kilo** es un agente AI integrado en VS Code. Para este lab, vamos a usarlo con el modelo **Code Supernova 1M**.\n",
        "\n",
        "```bash\n",
        "# En la terminal de VS Code:\n",
        "# (Los pasos exactos dependen de cómo tengan configurado Kilo en su institución)\n",
        "\n",
        "# Ejemplo conceptual:\n",
        "kilo config --model code-supernova-1m\n",
        "kilo auth login\n",
        "```\n",
        "\n",
        "**Tip**: Usá Kilo para:\n",
        "- Autocompletar código\n",
        "- Explicar funciones de MediaPipe\n",
        "- Refactorizar código del notebook\n",
        "- Generar docstrings\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azp1epl_Gmqy"
      },
      "source": [
        "### Fase 2: Creación del Entorno Virtual\n",
        "\n",
        "#### Paso 2.1: Crear el Proyecto\n",
        "\n",
        "```bash\n",
        "# Crear directorio del proyecto\n",
        "mkdir facial-landmarks-app\n",
        "cd facial-landmarks-app\n",
        "\n",
        "# Crear entorno virtual\n",
        "python -m venv venv\n",
        "\n",
        "# Activar el entorno virtual\n",
        "# En Windows:\n",
        "venv\\Scripts\\activate\n",
        "\n",
        "# En Linux/Mac:\n",
        "source venv/bin/activate\n",
        "```\n",
        "\n",
        "**Verificar activación**: Deberías ver `(venv)` antes del prompt en la terminal.\n",
        "\n",
        "#### Paso 2.2: Instalar Dependencias Base\n",
        "\n",
        "```bash\n",
        "# Instalar las bibliotecas necesarias\n",
        "pip install mediapipe opencv-python streamlit pillow numpy\n",
        "\n",
        "# Generar requirements.txt\n",
        "pip freeze > requirements.txt\n",
        "```\n",
        "\n",
        "**Contenido de `requirements.txt` esperado**:\n",
        "```\n",
        "mediapipe==0.10.21\n",
        "opencv-python==4.11.0.86\n",
        "streamlit==1.39.0\n",
        "Pillow==11.2.1\n",
        "numpy==1.26.4\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMa4iTciGmqy"
      },
      "source": [
        "### Fase 3: Conversión de Notebook a Scripts Python\n",
        "\n",
        "#### Paso 3.1: Analizar el Código del Notebook Original\n",
        "\n",
        "Del notebook `Landmarks_Faciales.ipynb`, identificamos estos componentes clave:\n",
        "\n",
        "```python\n",
        "# 1. INICIALIZACIÓN DEL MODELO\n",
        "mp_face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
        "    static_image_mode=True,\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.5\n",
        ")\n",
        "\n",
        "# 2. PROCESAMIENTO DE IMAGEN\n",
        "imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)\n",
        "resultados = mp_face_mesh.process(imagen_rgb)\n",
        "\n",
        "# 3. DIBUJO DE LANDMARKS\n",
        "for punto in rostro.landmark:\n",
        "    coord_x_pixel = int(punto.x * ancho)\n",
        "    coord_y_pixel = int(punto.y * alto)\n",
        "    cv2.circle(imagen_con_puntos, (coord_x_pixel, coord_y_pixel), 2, (0, 255, 0), -1)\n",
        "```\n",
        "\n",
        "#### Paso 3.2: Crear `src/config.py`\n",
        "\n",
        "```python\n",
        "# src/config.py\n",
        "\"\"\"\n",
        "Configuración del detector de landmarks faciales.\n",
        "\"\"\"\n",
        "\n",
        "# Parámetros del modelo MediaPipe\n",
        "FACE_MESH_CONFIG = {\n",
        "    \"static_image_mode\": True,\n",
        "    \"max_num_faces\": 1,\n",
        "    \"refine_landmarks\": True,\n",
        "    \"min_detection_confidence\": 0.5\n",
        "}\n",
        "\n",
        "# Configuración de visualización\n",
        "LANDMARK_COLOR = (0, 255, 0)  # Verde en BGR\n",
        "LANDMARK_RADIUS = 2\n",
        "LANDMARK_THICKNESS = -1  # Relleno\n",
        "\n",
        "# Cantidad de landmarks esperados\n",
        "TOTAL_LANDMARKS = 478\n",
        "```\n",
        "\n",
        "#### Paso 3.3: Crear `src/utils.py`\n",
        "\n",
        "```python\n",
        "# src/utils.py\n",
        "\"\"\"\n",
        "Funciones auxiliares para procesamiento de imágenes.\n",
        "\"\"\"\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def pil_to_cv2(pil_image):\n",
        "    \"\"\"\n",
        "    Convierte una imagen PIL a formato OpenCV (numpy array BGR).\n",
        "    \n",
        "    Args:\n",
        "        pil_image (PIL.Image): Imagen en formato PIL\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: Imagen en formato OpenCV (BGR)\n",
        "    \"\"\"\n",
        "    # Convertir PIL a RGB numpy array\n",
        "    rgb_array = np.array(pil_image.convert('RGB'))\n",
        "    # Convertir RGB a BGR (formato OpenCV)\n",
        "    bgr_array = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2BGR)\n",
        "    return bgr_array\n",
        "\n",
        "\n",
        "def cv2_to_pil(cv2_image):\n",
        "    \"\"\"\n",
        "    Convierte una imagen OpenCV a formato PIL.\n",
        "    \n",
        "    Args:\n",
        "        cv2_image (numpy.ndarray): Imagen en formato OpenCV (BGR)\n",
        "    \n",
        "    Returns:\n",
        "        PIL.Image: Imagen en formato PIL (RGB)\n",
        "    \"\"\"\n",
        "    # Convertir BGR a RGB\n",
        "    rgb_array = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
        "    # Convertir a PIL\n",
        "    pil_image = Image.fromarray(rgb_array)\n",
        "    return pil_image\n",
        "\n",
        "\n",
        "def resize_image(image, max_width=800):\n",
        "    \"\"\"\n",
        "    Redimensiona la imagen manteniendo el aspect ratio.\n",
        "    \n",
        "    Args:\n",
        "        image (numpy.ndarray): Imagen OpenCV\n",
        "        max_width (int): Ancho máximo deseado\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: Imagen redimensionada\n",
        "    \"\"\"\n",
        "    alto, ancho = image.shape[:2]\n",
        "    \n",
        "    if ancho > max_width:\n",
        "        ratio = max_width / ancho\n",
        "        nuevo_ancho = max_width\n",
        "        nuevo_alto = int(alto * ratio)\n",
        "        image = cv2.resize(image, (nuevo_ancho, nuevo_alto))\n",
        "    \n",
        "    return image\n",
        "```\n",
        "\n",
        "#### Paso 3.4: Crear `src/detector.py`\n",
        "\n",
        "```python\n",
        "# src/detector.py\n",
        "\"\"\"\n",
        "Detector de landmarks faciales usando MediaPipe.\n",
        "\"\"\"\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from .config import FACE_MESH_CONFIG, LANDMARK_COLOR, LANDMARK_RADIUS, LANDMARK_THICKNESS\n",
        "\n",
        "\n",
        "class FaceLandmarkDetector:\n",
        "    \"\"\"\n",
        "    Clase para detectar y visualizar landmarks faciales.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Inicializa el detector de MediaPipe.\"\"\"\n",
        "        self.face_mesh = mp.solutions.face_mesh.FaceMesh(**FACE_MESH_CONFIG)\n",
        "        self.mp_drawing = mp.solutions.drawing_utils\n",
        "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
        "    \n",
        "    def detect(self, image):\n",
        "        \"\"\"\n",
        "        Detecta landmarks faciales en la imagen.\n",
        "        \n",
        "        Args:\n",
        "            image (numpy.ndarray): Imagen en formato BGR (OpenCV)\n",
        "        \n",
        "        Returns:\n",
        "            tuple: (imagen_procesada, landmarks, info)\n",
        "                - imagen_procesada: imagen con landmarks dibujados\n",
        "                - landmarks: objeto de landmarks de MediaPipe\n",
        "                - info: diccionario con información de detección\n",
        "        \"\"\"\n",
        "        # Convertir BGR a RGB para MediaPipe\n",
        "        imagen_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Procesar la imagen\n",
        "        resultados = self.face_mesh.process(imagen_rgb)\n",
        "        \n",
        "        # Crear copia para dibujar\n",
        "        imagen_con_puntos = image.copy()\n",
        "        \n",
        "        info = {\n",
        "            \"rostros_detectados\": 0,\n",
        "            \"total_landmarks\": 0,\n",
        "            \"deteccion_exitosa\": False\n",
        "        }\n",
        "        \n",
        "        # Si se detectaron rostros\n",
        "        if resultados.multi_face_landmarks:\n",
        "            info[\"rostros_detectados\"] = len(resultados.multi_face_landmarks)\n",
        "            \n",
        "            # Tomar el primer rostro\n",
        "            rostro = resultados.multi_face_landmarks[0]\n",
        "            info[\"total_landmarks\"] = len(rostro.landmark)\n",
        "            info[\"deteccion_exitosa\"] = True\n",
        "            \n",
        "            # Dibujar landmarks\n",
        "            alto, ancho = image.shape[:2]\n",
        "            \n",
        "            for punto in rostro.landmark:\n",
        "                coord_x_pixel = int(punto.x * ancho)\n",
        "                coord_y_pixel = int(punto.y * alto)\n",
        "                \n",
        "                cv2.circle(\n",
        "                    imagen_con_puntos,\n",
        "                    (coord_x_pixel, coord_y_pixel),\n",
        "                    LANDMARK_RADIUS,\n",
        "                    LANDMARK_COLOR,\n",
        "                    LANDMARK_THICKNESS\n",
        "                )\n",
        "            \n",
        "            return imagen_con_puntos, rostro, info\n",
        "        \n",
        "        # No se detectó rostro\n",
        "        return imagen_con_puntos, None, info\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Libera recursos del detector.\"\"\"\n",
        "        self.face_mesh.close()\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKGRcTCJGmqz"
      },
      "source": [
        "### Fase 4: Desarrollo de la Interfaz Streamlit\n",
        "\n",
        "#### Paso 4.1: Crear `app.py`\n",
        "\n",
        "```python\n",
        "# app.py\n",
        "\"\"\"\n",
        "Aplicación Streamlit para detección de landmarks faciales.\n",
        "\"\"\"\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "from src.detector import FaceLandmarkDetector\n",
        "from src.utils import pil_to_cv2, cv2_to_pil, resize_image\n",
        "from src.config import TOTAL_LANDMARKS\n",
        "\n",
        "\n",
        "# Configuración de la página\n",
        "st.set_page_config(\n",
        "    page_title=\"Detector de Landmarks Faciales\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Título y descripción\n",
        "st.title(\"Detector de Landmarks Faciales\")\n",
        "st.markdown(\"\"\"\n",
        "Esta aplicación detecta **478 puntos clave** en rostros humanos usando MediaPipe.\n",
        "Subí una imagen con un rostro y mirá la magia de la visión por computadora.\n",
        "\"\"\")\n",
        "\n",
        "# Sidebar con información\n",
        "with st.sidebar:\n",
        "    st.header(\"Información\")\n",
        "    st.markdown(\"\"\"\n",
        "    ### ¿Qué son los Landmarks?\n",
        "    Son puntos de referencia que mapean:\n",
        "    - Ojos (iris, párpados)\n",
        "    - Nariz (puente, fosas)\n",
        "    - Boca (labios, comisuras)\n",
        "    - Contorno facial\n",
        "    \n",
        "    ### Aplicaciones\n",
        "    - Filtros AR (Instagram)\n",
        "    - Análisis de expresiones\n",
        "    - Animación facial\n",
        "    - Autenticación biométrica\n",
        "    \"\"\")\n",
        "    \n",
        "    st.divider()\n",
        "    st.caption(\"Desarrollado en el Laboratorio 2 - IFTS24\")\n",
        "\n",
        "# Uploader de imagen\n",
        "uploaded_file = st.file_uploader(\n",
        "    \"Subí una imagen con un rostro\",\n",
        "    type=[\"jpg\", \"jpeg\", \"png\"],\n",
        "    help=\"Formatos aceptados: JPG, JPEG, PNG\"\n",
        ")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Cargar imagen\n",
        "    imagen_original = Image.open(uploaded_file)\n",
        "    \n",
        "    # Convertir a formato OpenCV\n",
        "    imagen_cv2 = pil_to_cv2(imagen_original)\n",
        "    \n",
        "    # Redimensionar si es muy grande\n",
        "    imagen_cv2 = resize_image(imagen_cv2, max_width=800)\n",
        "    \n",
        "    # Columnas para mostrar antes/después\n",
        "    col1, col2 = st.columns(2)\n",
        "    \n",
        "    with col1:\n",
        "        st.subheader(\"Imagen Original\")\n",
        "        st.image(cv2_to_pil(imagen_cv2), use_container_width=True)\n",
        "    \n",
        "    # Detectar landmarks\n",
        "    with st.spinner(\"Detectando landmarks faciales...\"):\n",
        "        detector = FaceLandmarkDetector()\n",
        "        imagen_procesada, landmarks, info = detector.detect(imagen_cv2)\n",
        "        detector.close()\n",
        "    \n",
        "    with col2:\n",
        "        st.subheader(\"Landmarks Detectados\")\n",
        "        st.image(cv2_to_pil(imagen_procesada), use_container_width=True)\n",
        "    \n",
        "    # Mostrar información de detección\n",
        "    st.divider()\n",
        "    \n",
        "    if info[\"deteccion_exitosa\"]:\n",
        "        st.success(\"Detección exitosa\")\n",
        "        \n",
        "        # Métricas\n",
        "        metric_col1, metric_col2, metric_col3 = st.columns(3)\n",
        "        \n",
        "        with metric_col1:\n",
        "            st.metric(\"Rostros detectados\", info[\"rostros_detectados\"])\n",
        "        \n",
        "        with metric_col2:\n",
        "            st.metric(\"Landmarks detectados\", f\"{info['total_landmarks']}/{TOTAL_LANDMARKS}\")\n",
        "        \n",
        "        with metric_col3:\n",
        "            porcentaje = (info['total_landmarks'] / TOTAL_LANDMARKS) * 100\n",
        "            st.metric(\"Precisión\", f\"{porcentaje:.1f}%\")\n",
        "    else:\n",
        "        st.error(\"No se detectó ningún rostro en la imagen\")\n",
        "        st.info(\"\"\"\n",
        "        **Consejos**:\n",
        "        - Asegurate de que el rostro esté bien iluminado\n",
        "        - El rostro debe estar mirando hacia la cámara\n",
        "        - Probá con una imagen de mayor calidad\n",
        "        \"\"\")\n",
        "\n",
        "else:\n",
        "    # Mensaje de bienvenida\n",
        "    st.info(\"Subí una imagen para comenzar la detección\")\n",
        "    \n",
        "    # Ejemplo visual\n",
        "    st.markdown(\"### Ejemplo de Resultado\")\n",
        "    st.image(\n",
        "        \"https://ai.google.dev/static/mediapipe/images/solutions/face_landmarker_keypoints.png?hl=es-419\",\n",
        "        caption=\"MediaPipe detecta 478 landmarks faciales\",\n",
        "        width=400\n",
        "    )\n",
        "```\n",
        "\n",
        "#### Paso 4.2: Probar la Aplicación Localmente\n",
        "\n",
        "```bash\n",
        "# En la terminal con el venv activado:\n",
        "streamlit run app.py\n",
        "```\n",
        "\n",
        "Esto debería abrir automáticamente `http://localhost:8501` en tu navegador.\n",
        "\n",
        "**Checklist de Testing**:\n",
        "- [ ] La aplicación carga sin errores\n",
        "- [ ] Podés subir una imagen\n",
        "- [ ] Se detectan los landmarks correctamente\n",
        "- [ ] Las métricas se muestran bien\n",
        "- [ ] El layout es responsive\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwAbGob4Gmq0"
      },
      "source": [
        "### Fase 5: Preparación para Deployment\n",
        "\n",
        "#### Paso 5.1: Crear `.gitignore`\n",
        "\n",
        "```gitignore\n",
        "# .gitignore\n",
        "\n",
        "# Entorno virtual\n",
        "venv/\n",
        "env/\n",
        ".venv/\n",
        "\n",
        "# Python\n",
        "__pycache__/\n",
        "*.py[cod]\n",
        "*$py.class\n",
        "*.so\n",
        ".Python\n",
        "\n",
        "# Jupyter\n",
        ".ipynb_checkpoints\n",
        "\n",
        "# IDE\n",
        ".vscode/\n",
        ".idea/\n",
        "\n",
        "# Sistema\n",
        ".DS_Store\n",
        "Thumbs.db\n",
        "\n",
        "# Streamlit\n",
        ".streamlit/secrets.toml\n",
        "```\n",
        "\n",
        "#### Paso 5.2: Crear `README.md`\n",
        "\n",
        "```markdown\n",
        "# Detector de Landmarks Faciales\n",
        "\n",
        "Aplicación web para detectar 478 puntos clave en rostros humanos usando MediaPipe y Streamlit.\n",
        "\n",
        "## Características\n",
        "\n",
        "- Detección de 478 landmarks faciales\n",
        "- Interfaz web interactiva\n",
        "- Procesamiento en tiempo real\n",
        "- Visualización antes/después\n",
        "\n",
        "## Tecnologías\n",
        "\n",
        "- **MediaPipe**: Detección de landmarks\n",
        "- **OpenCV**: Procesamiento de imágenes\n",
        "- **Streamlit**: Framework web\n",
        "- **Python 3.11+**\n",
        "\n",
        "## Instalación Local\n",
        "\n",
        "```bash\n",
        "# Clonar repositorio\n",
        "git clone https://github.com/tu-usuario/facial-landmarks-app.git\n",
        "cd facial-landmarks-app\n",
        "\n",
        "# Crear entorno virtual\n",
        "python -m venv venv\n",
        "venv\\Scripts\\activate  # Windows\n",
        "source venv/bin/activate  # Linux/Mac\n",
        "\n",
        "# Instalar dependencias\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# Ejecutar aplicación\n",
        "streamlit run app.py\n",
        "```\n",
        "\n",
        "## Deployment en Streamlit Community Cloud\n",
        "\n",
        "1. Subir el código a GitHub\n",
        "2. Ir a [https://share.streamlit.io](https://share.streamlit.io)\n",
        "3. Conectar tu repositorio\n",
        "4. Configurar el archivo principal como `app.py`\n",
        "5. Deploy\n",
        "\n",
        "## Documentación\n",
        "\n",
        "- [MediaPipe Face Landmarker](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker)\n",
        "- [Streamlit Docs](https://docs.streamlit.io)\n",
        "- [Kilo Code](https://kilocode.ai/)\n",
        "\n",
        "## Autor\n",
        "\n",
        "Desarrollado como parte del Laboratorio 2 - IFTS24  \n",
        "Materia: Procesamiento Digital de Imágenes\n",
        "\n",
        "## Licencia\n",
        "\n",
        "MIT License\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NSq7zR5Gmq0"
      },
      "source": [
        "### Fase 6: Deployment en Streamlit Community Cloud\n",
        "\n",
        "#### Paso 6.1: Preparar el Repositorio Git\n",
        "\n",
        "```bash\n",
        "# Inicializar repositorio\n",
        "git init\n",
        "git add .\n",
        "git commit -m \"Initial commit: Facial landmarks detector\"\n",
        "\n",
        "# Crear repositorio en GitHub (desde la web)\n",
        "# Luego conectar:\n",
        "git remote add origin https://github.com/tu-usuario/facial-landmarks-app.git\n",
        "git branch -M main\n",
        "git push -u origin main\n",
        "```\n",
        "\n",
        "#### Paso 6.2: Deployment en Streamlit Community Cloud\n",
        "\n",
        "**Sitio oficial**: [https://streamlit.io/](https://streamlit.io/)\n",
        "\n",
        "**Pasos para desplegar**:\n",
        "\n",
        "1. **Ir a** [https://share.streamlit.io](https://share.streamlit.io)\n",
        "2. **Conectar** tu cuenta de GitHub\n",
        "3. **Seleccionar** tu repositorio `facial-landmarks-app`\n",
        "4. **Configurar**:\n",
        "   - Branch: `main`\n",
        "   - Main file path: `app.py`\n",
        "5. **Deploy**\n",
        "\n",
        "Tu app estará disponible en: `https://tu-usuario-facial-landmarks-app.streamlit.app`\n",
        "\n",
        "#### Paso 6.3: (Alternativa) Deployment en Hugging Face Spaces\n",
        "\n",
        "Si preferís usar Hugging Face (del Lab 1):\n",
        "\n",
        "1. Crear un nuevo Space en [https://huggingface.co/spaces](https://huggingface.co/spaces)\n",
        "2. Elegir SDK: **Streamlit**\n",
        "3. Subir los archivos:\n",
        "   - `app.py`\n",
        "   - `requirements.txt`\n",
        "   - Carpeta `src/`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTT5V2vlGmq1"
      },
      "source": [
        "## Parte 4: Actividades de Aprendizaje\n",
        "\n",
        "**Fecha de entrega**: 29 de octubre de 2025\n",
        "\n",
        "---\n",
        "\n",
        "### Actividad 1: Implementación Base (Obligatoria)\n",
        "\n",
        "**Objetivo**: Crear la aplicación funcional siguiendo la guía.\n",
        "\n",
        "**Entregables**:\n",
        "1. Repositorio GitHub con el código\n",
        "2. Aplicación desplegada y funcionando\n",
        "3. Screenshot de la aplicación detectando landmarks en tu propia foto\n",
        "\n",
        "**Criterios de evaluación**:\n",
        "- Estructura de directorios correcta\n",
        "- Código modularizado en `src/`\n",
        "- Aplicación funcionando localmente\n",
        "- Deployment exitoso\n",
        "- README.md completo\n",
        "\n",
        "---\n",
        "\n",
        "### Actividad 2: Mejoras y Extensiones (Optativas)\n",
        "\n",
        "Elegí **al menos 2** de estas mejoras:\n",
        "\n",
        "#### Opción A: Múltiples Estilos de Visualización\n",
        "\n",
        "Agregá opciones para visualizar los landmarks de diferentes formas:\n",
        "- Puntos + malla conectada (usando `mp.solutions.drawing_utils`)\n",
        "- Solo contornos principales (ojos, boca, rostro)\n",
        "- Heatmap de densidad de puntos\n",
        "\n",
        "**Tip con Kilo**: Pedile que te ayude a usar `mp_drawing.draw_landmarks()`\n",
        "\n",
        "```python\n",
        "# Ejemplo para empezar:\n",
        "mp_drawing.draw_landmarks(\n",
        "    image=imagen_con_puntos,\n",
        "    landmark_list=rostro,\n",
        "    connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
        "    landmark_drawing_spec=None,\n",
        "    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
        ")\n",
        "```\n",
        "\n",
        "#### Opción B: Análisis de Expresiones\n",
        "\n",
        "Calculá métricas simples basadas en landmarks:\n",
        "- Apertura de boca (distancia entre labios)\n",
        "- Apertura de ojos\n",
        "- Inclinación de cabeza\n",
        "\n",
        "```python\n",
        "# Ejemplo para distancia entre labios:\n",
        "import math\n",
        "\n",
        "def calcular_apertura_boca(landmarks, alto, ancho):\n",
        "    # Landmark 13: labio superior\n",
        "    # Landmark 14: labio inferior\n",
        "    punto_superior = landmarks.landmark[13]\n",
        "    punto_inferior = landmarks.landmark[14]\n",
        "    \n",
        "    y1 = punto_superior.y * alto\n",
        "    y2 = punto_inferior.y * alto\n",
        "    \n",
        "    distancia = abs(y2 - y1)\n",
        "    return distancia\n",
        "```\n",
        "\n",
        "#### Opción C: Procesamiento de Video\n",
        "\n",
        "Modificá el código para procesar videos frame por frame:\n",
        "\n",
        "```python\n",
        "# Cambiar en config.py:\n",
        "FACE_MESH_CONFIG = {\n",
        "    \"static_image_mode\": False,  # Cambio clave\n",
        "    \"max_num_faces\": 1,\n",
        "    \"refine_landmarks\": True,\n",
        "    \"min_detection_confidence\": 0.5,\n",
        "    \"min_tracking_confidence\": 0.5  # Nuevo parámetro\n",
        "}\n",
        "```\n",
        "\n",
        "Investigá `st.camera_input()` para captura en vivo.\n",
        "\n",
        "#### Opción D: Exportar Landmarks a JSON\n",
        "\n",
        "Agregá un botón para descargar las coordenadas de los landmarks:\n",
        "\n",
        "```python\n",
        "import json\n",
        "\n",
        "def landmarks_to_dict(landmarks, alto, ancho):\n",
        "    \"\"\"Convierte landmarks a diccionario exportable.\"\"\"\n",
        "    data = []\n",
        "    for idx, punto in enumerate(landmarks.landmark):\n",
        "        data.append({\n",
        "            \"id\": idx,\n",
        "            \"x\": punto.x * ancho,\n",
        "            \"y\": punto.y * alto,\n",
        "            \"z\": punto.z\n",
        "        })\n",
        "    return data\n",
        "\n",
        "# En Streamlit:\n",
        "landmarks_json = json.dumps(landmarks_to_dict(landmarks, alto, ancho), indent=2)\n",
        "st.download_button(\n",
        "    label=\"Descargar Landmarks (JSON)\",\n",
        "    data=landmarks_json,\n",
        "    file_name=\"landmarks.json\",\n",
        "    mime=\"application/json\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Actividad 3: Documentación Técnica (Obligatoria)\n",
        "\n",
        "Creá un documento `INFORME.md` que incluya:\n",
        "\n",
        "1. **Introducción**: Explicá qué son los landmarks y su importancia\n",
        "2. **Arquitectura**: Diagrama de la estructura del proyecto\n",
        "3. **Decisiones de diseño**: Por qué elegiste cierta estructura de código\n",
        "4. **Desafíos**: Problemas que encontraste y cómo los resolviste\n",
        "5. **Uso de Kilo**: Capturas de cómo usaste el agente AI durante el desarrollo\n",
        "6. **Conclusiones**: Aprendizajes principales\n",
        "\n",
        "**Formato**: Markdown con imágenes, código y referencias.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkWbcSziGmq1"
      },
      "source": [
        "## Parte 5: Guía de Uso de Kilo como Agente\n",
        "\n",
        "**Sitio oficial de Kilo**: [https://kilocode.ai/](https://kilocode.ai/)\n",
        "\n",
        "### ¿Cómo Aprovechar Kilo en Este Proyecto?\n",
        "\n",
        "Kilo puede ser tu asistente de desarrollo en cada fase:\n",
        "\n",
        "#### 1. Conversión de Notebook a Scripts\n",
        "\n",
        "**Prompt para Kilo**:\n",
        "```\n",
        "Tengo este código de Jupyter notebook:\n",
        "[pegar código de detección]\n",
        "\n",
        "Necesito convertirlo en una clase Python con:\n",
        "- Método __init__ para configurar MediaPipe\n",
        "- Método detect() que reciba una imagen y retorne landmarks\n",
        "- Docstrings en español\n",
        "- Type hints\n",
        "```\n",
        "\n",
        "#### 2. Debugging de Errores\n",
        "\n",
        "**Ejemplo de uso**:\n",
        "```python\n",
        "# Si te sale este error:\n",
        "# AttributeError: module 'cv2' has no attribute 'cvtColor'\n",
        "\n",
        "# Preguntale a Kilo:\n",
        "# \"Tengo este error en OpenCV, ¿qué puede estar pasando?\"\n",
        "# [pegar traceback completo]\n",
        "```\n",
        "\n",
        "#### 3. Mejoras de Código\n",
        "\n",
        "**Prompt para refactoring**:\n",
        "```\n",
        "Refactorizá esta función para que:\n",
        "- Maneje excepciones correctamente\n",
        "- Valide inputs\n",
        "- Sea más eficiente\n",
        "\n",
        "[pegar función]\n",
        "```\n",
        "\n",
        "#### 4. Explicación de Conceptos\n",
        "\n",
        "```\n",
        "\"Explicame cómo funciona la detección de landmarks en MediaPipe.\n",
        "Necesito entenderlo para documentar mi código.\"\n",
        "```\n",
        "\n",
        "#### 5. Generación de Tests\n",
        "\n",
        "```\n",
        "\"Generá tests unitarios para la clase FaceLandmarkDetector usando pytest.\n",
        "Necesito testear:\n",
        "- Detección exitosa con imagen válida\n",
        "- Manejo de imagen sin rostro\n",
        "- Conversión de formatos\"\n",
        "```\n",
        "\n",
        "### Comandos Útiles de Kilo en VS Code\n",
        "\n",
        "- **Ctrl + I**: Abrir chat de Kilo\n",
        "- **Seleccionar código + Ctrl + I**: Preguntar sobre código específico\n",
        "- **Cmd/Ctrl + K**: Autocompletar código\n",
        "\n",
        "### Mejores Prácticas con Agentes AI\n",
        "\n",
        "**SÍ hacer**:\n",
        "- Dar contexto completo del problema\n",
        "- Pedir explicaciones de código generado\n",
        "- Usar para aprender patrones de diseño\n",
        "- Validar siempre el código generado\n",
        "\n",
        "**NO hacer**:\n",
        "- Copiar código sin entenderlo\n",
        "- Depender 100% del agente\n",
        "- Aceptar código sin testearlo\n",
        "- Ignorar warnings de seguridad\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rphQL_9vGmq2"
      },
      "source": [
        "## Parte 6: Checklist de Entrega Final\n",
        "\n",
        "### Estructura del Proyecto\n",
        "\n",
        "```\n",
        "facial-landmarks-app/\n",
        "├── venv/ (no subir a Git)\n",
        "├── src/\n",
        "│   ├── __init__.py\n",
        "│   ├── detector.py\n",
        "│   ├── utils.py\n",
        "│   └── config.py\n",
        "├── app.py\n",
        "├── requirements.txt\n",
        "├── .gitignore\n",
        "├── README.md\n",
        "└── INFORME.md (tu documentación)\n",
        "```\n",
        "\n",
        "### Funcionalidades Mínimas\n",
        "\n",
        "- [ ] Subir imagen desde interfaz\n",
        "- [ ] Detectar landmarks correctamente\n",
        "- [ ] Mostrar imagen antes/después\n",
        "- [ ] Mostrar métricas (cantidad de landmarks, rostros detectados)\n",
        "- [ ] Manejar casos sin detección de rostro\n",
        "- [ ] Sidebar con información educativa\n",
        "- [ ] Aplicación desplegada online\n",
        "\n",
        "### Documentación Requerida\n",
        "\n",
        "- [ ] README.md con instrucciones de instalación\n",
        "- [ ] Docstrings en todas las funciones\n",
        "- [ ] Comentarios explicativos en código complejo\n",
        "- [ ] INFORME.md con análisis técnico\n",
        "- [ ] Screenshots de la aplicación funcionando\n",
        "\n",
        "### Calidad de Código\n",
        "\n",
        "- [ ] Código modularizado (no todo en un archivo)\n",
        "- [ ] Nombres de variables descriptivos en español\n",
        "- [ ] Manejo de excepciones\n",
        "- [ ] No hay código duplicado\n",
        "- [ ] Configuración separada de lógica (config.py)\n",
        "\n",
        "### Deployment\n",
        "\n",
        "- [ ] Repositorio Git público\n",
        "- [ ] Aplicación accesible vía URL\n",
        "- [ ] README incluye link a app desplegada\n",
        "- [ ] No hay errores en logs de deployment\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkE0DN5jGmq2"
      },
      "source": [
        "## Recursos Adicionales\n",
        "\n",
        "### Documentación Oficial\n",
        "\n",
        "- [MediaPipe Face Mesh Guide](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker?hl=es-419)\n",
        "- [Streamlit API Reference](https://docs.streamlit.io/library/api-reference)\n",
        "- [OpenCV Python Tutorials](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)\n",
        "- [Python venv Documentation](https://docs.python.org/3/library/venv.html)\n",
        "\n",
        "### Tutoriales Recomendados\n",
        "\n",
        "- [Streamlit from Scratch](https://www.youtube.com/watch?v=JwSS70SZdyM)\n",
        "- [MediaPipe Hands-On](https://www.youtube.com/watch?v=wyWmWaXapmI)\n",
        "- [VS Code for Python](https://code.visualstudio.com/docs/python/python-tutorial)\n",
        "\n",
        "### Inspiración de Proyectos\n",
        "\n",
        "- [Face Mesh AR Filters](https://github.com/google/mediapipe/tree/master/mediapipe/examples)\n",
        "- [Emotion Detection App](https://huggingface.co/spaces/nlpconnect/emotion-detection)\n",
        "- [Streamlit Gallery](https://streamlit.io/gallery)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cum27xjJGmq2"
      },
      "source": [
        "## Preguntas Frecuentes (FAQ)\n",
        "\n",
        "### 1. ¿Por qué MediaPipe no detecta mi rostro?\n",
        "\n",
        "**Posibles causas**:\n",
        "- Imagen muy oscura o borrosa\n",
        "- Rostro de perfil (debe estar frontal)\n",
        "- Resolución muy baja\n",
        "- Rostro parcialmente cubierto\n",
        "\n",
        "**Solución**: Ajustá `min_detection_confidence` en `config.py` a un valor más bajo (ej: 0.3).\n",
        "\n",
        "### 2. ¿Cómo convierto el notebook si tengo código muy complejo?\n",
        "\n",
        "**Estrategia**:\n",
        "1. Identificá bloques de funcionalidad (detección, visualización, procesamiento)\n",
        "2. Convertí cada bloque en una función\n",
        "3. Agrupá funciones relacionadas en módulos\n",
        "4. Usá Kilo para refactorizar paso a paso\n",
        "\n",
        "### 3. ¿Streamlit es gratis?\n",
        "\n",
        "Sí, **Streamlit Community Cloud** es gratis para:\n",
        "- Repositorios públicos\n",
        "- Hasta 1GB de recursos\n",
        "- Apps ilimitadas\n",
        "\n",
        "### 4. ¿Puedo usar otra interfaz en lugar de Streamlit?\n",
        "\n",
        "Sí, pero deberías consultar antes. Alternativas:\n",
        "- **Gradio**: Más simple, menos personalizable\n",
        "- **Flask**: Más control, más complejidad\n",
        "- **FastAPI + HTML**: Para APIs REST\n",
        "\n",
        "### 5. ¿Qué hago si el deployment falla?\n",
        "\n",
        "**Checklist de troubleshooting**:\n",
        "1. Verificá que `requirements.txt` esté completo\n",
        "2. Asegurate de que no haya rutas absolutas en el código\n",
        "3. Revisá los logs de deployment\n",
        "4. Comprobá que el `app.py` esté en la raíz\n",
        "5. Verificá compatibilidad de versiones de librerías\n",
        "\n",
        "### 6. ¿Cómo mejoro el rendimiento?\n",
        "\n",
        "**Optimizaciones**:\n",
        "```python\n",
        "# 1. Cachear el modelo\n",
        "@st.cache_resource\n",
        "def load_detector():\n",
        "    return FaceLandmarkDetector()\n",
        "\n",
        "# 2. Redimensionar imágenes grandes\n",
        "imagen_cv2 = resize_image(imagen_cv2, max_width=800)\n",
        "\n",
        "# 3. Usar static_image_mode=True para imágenes\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm-QLlIhGmq2"
      },
      "source": [
        "## Criterios de Evaluación\n",
        "\n",
        "### Rúbrica de Evaluación (Total: 100 puntos)\n",
        "\n",
        "| Criterio | Puntos | Descripción |\n",
        "|----------|--------|-------------|\n",
        "| **Funcionalidad** | 30 | La aplicación detecta landmarks correctamente y maneja errores |\n",
        "| **Estructura de Código** | 20 | Código modularizado, limpio y organizado en `src/` |\n",
        "| **Documentación** | 15 | README, docstrings, comentarios e informe técnico |\n",
        "| **Deployment** | 15 | Aplicación desplegada y accesible online |\n",
        "| **Uso de Herramientas** | 10 | Uso efectivo de VS Code, venv, Git y Kilo |\n",
        "| **Mejoras Opcionales** | 10 | Implementación de al menos 2 extensiones |\n",
        "\n",
        "### Niveles de Logro\n",
        "\n",
        "**Excelente (90-100)**:\n",
        "- Todas las funcionalidades base implementadas\n",
        "- Al menos 2 mejoras opcionales\n",
        "- Código profesional y bien documentado\n",
        "- Deployment exitoso con 0 errores\n",
        "- Informe técnico detallado con análisis profundo\n",
        "\n",
        "**Muy Bueno (75-89)**:\n",
        "- Funcionalidades base completas\n",
        "- 1 mejora opcional\n",
        "- Código limpio con documentación básica\n",
        "- Deployment exitoso\n",
        "- Informe técnico completo\n",
        "\n",
        "**Bueno (60-74)**:\n",
        "- Funcionalidades base implementadas\n",
        "- Sin mejoras opcionales\n",
        "- Código funcional pero con áreas de mejora\n",
        "- Deployment con algunos warnings\n",
        "- Informe básico\n",
        "\n",
        "**Aprobado (50-59)**:\n",
        "- Algunas funcionalidades faltantes\n",
        "- Sin mejoras opcionales\n",
        "- Código desorganizado\n",
        "- Deployment con errores menores\n",
        "- Documentación incompleta\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6RrQuUgGmq3"
      },
      "source": [
        "## Conclusión\n",
        "\n",
        "Este laboratorio te lleva desde un **notebook exploratorio** hasta una **aplicación web en producción**, pasando por todas las etapas del ciclo de desarrollo profesional:\n",
        "\n",
        "1. **Prototipado** en Jupyter\n",
        "2. **Modularización** en scripts Python\n",
        "3. **Interfaz de usuario** con Streamlit\n",
        "4. **Control de versiones** con Git\n",
        "5. **Deployment** en la nube\n",
        "\n",
        "Aprendiste a usar herramientas profesionales:\n",
        "- VS Code como IDE principal\n",
        "- Kilo como asistente AI de desarrollo\n",
        "- Entornos virtuales para gestión de dependencias\n",
        "- Plataformas cloud para deployment\n",
        "\n",
        "Y aplicaste conceptos clave de visión por computadora:\n",
        "- Detección de landmarks faciales\n",
        "- Procesamiento de imágenes con OpenCV\n",
        "- Modelos pre-entrenados (MediaPipe)\n",
        "\n",
        "### Próximos Pasos\n",
        "\n",
        "Podés extender este proyecto para:\n",
        "- Detectar expresiones faciales (feliz, triste, sorprendido)\n",
        "- Aplicar filtros AR en tiempo real\n",
        "- Analizar movimientos faciales para aplicaciones médicas\n",
        "- Controlar interfaces con gestos faciales\n",
        "\n",
        "**Éxitos con el laboratorio**\n",
        "\n",
        "---\n",
        "\n",
        "**Fecha de entrega**: 29 de octubre de 2025  \n",
        "**Consultas**: [Email/Canal de comunicación]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}